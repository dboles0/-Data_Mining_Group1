{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Import\n",
    "\n",
    "* working localy trim the data to ensure you can work with it locally <br\\>*(my computer does not have sufficient local memory to join all tables so I use appropriate attributes)*\n",
    "\n",
    "attempted stratagy (merge pd_df until memory bound then print to file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json, csv, os\n",
    "import pandas as pd\n",
    "\n",
    "def JSON_to_dataFrame(file): \n",
    "    \n",
    "    fp = open(file, encoding=\"utf8\")\n",
    "    json_obj = [ json.loads(x) for x in fp.readlines() ] \n",
    "    df = pd.DataFrame(json_obj)\n",
    "    fp.close\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "root_path = \"/Users/2015mbp16gb256gb/Documents/school/Fall_2019/4502-Data_Mining/Group_Project/Group1/yelp_dataset/\"\n",
    "rbu_path = root_path + \"rbu_merge.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 49s, sys: 2min 5s, total: 4min 55s\n",
      "Wall time: 5min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "review_df = JSON_to_dataFrame(root_path + \"review.json\")\n",
    "business_df = JSON_to_dataFrame(root_path + \"business.json\")\n",
    "user_df = JSON_to_dataFrame(root_path + \"user.json\")\n",
    "# tip_df = JSON_to_dataFrame(root_path + \"tip.json\")                     /not used\n",
    "# checkin_df = JSON_to_dataFrame(root_path + \"checkin.json\")             /not used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*********\n",
    "### Merging Files\n",
    "* paired the information down by combining the review business and user json files in to one merged csv file for processing (rbu_merged.csv) (deleted the other files and will be using this one to save space on my computer, if we need additional columns we will have to run this over adding the column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# utility function to make a dict of columns with table name added\n",
    "# used for the renaming of column names\n",
    "def make_columns(lst, base):\n",
    "    ret = {}\n",
    "    for i in lst:\n",
    "        ret[i] = base + '_' + i\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# return dataframe with column names updated to \"table + _column_name\"\n",
    "def process_df(a_df, table): \n",
    "\n",
    "    col_list = [i for i in a_df]\n",
    "    new_dict = make_columns(col_list, table)\n",
    "    return a_df.rename(columns=new_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.15 s, sys: 20.5 s, total: 26.6 s\n",
      "Wall time: 36 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# review col = ['business_id', 'cool', 'date', 'funny', 'review_id', 'stars', 'text', 'useful', 'user_id']\n",
    "\n",
    "# ***Update review_df*** \n",
    "review_drop = ['cool','funny']\n",
    "review_df = review_df.drop(review_drop, axis=1)\n",
    "\n",
    "review_df = process_df(review_df, 'review')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 602 ms, sys: 1 s, total: 1.6 s\n",
      "Wall time: 2.03 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# business col = \n",
    "# ['address','attributes','business_id','categories','city','hours','is_open',              \n",
    "# 'latitude','longitude','name','postal_code','review_count','stars','state']                              \n",
    "\n",
    "# ***Update business_df***\n",
    "business_drop = ['address', 'attributes', 'categories','city', 'postal_code']\n",
    "business_df = business_df.drop(business_drop, axis=1)\n",
    "\n",
    "business_df = process_df(business_df, 'business')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29.4 s, sys: 1.63 s, total: 31 s\n",
      "Wall time: 31.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "rb_merge = pd.merge(review_df, business_df, left_on='review_business_id', right_on='business_business_id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.5 s, sys: 6.57 s, total: 10.1 s\n",
      "Wall time: 10.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# user col = \n",
    "# ['average_stars','compliment_cool','compliment_cute','compliment_funny','compliment_hot','compliment_list',           \n",
    "# 'compliment_more','compliment_note','compliment_photos','compliment_plain','compliment_profile','compliment_writer', \n",
    "# 'cool','elite','fans','friends','funny','name','review_count','useful','user_id','yelping_since']   \n",
    "\n",
    "# ***Update _user_df***\n",
    "user_drop = ['average_stars','compliment_cool','compliment_cute','compliment_funny','compliment_hot','compliment_list','compliment_more','compliment_note','compliment_photos','compliment_plain','compliment_profile','compliment_writer','cool', 'elite', 'fans','friends','funny']\n",
    "user_df = user_df.drop(user_drop, axis=1)\n",
    "\n",
    "user_df = process_df(user_df, 'user')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25 s, sys: 7.73 s, total: 32.7 s\n",
      "Wall time: 34.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "rbu_merge = pd.merge(rb_merge, user_df, left_on='review_user_id', right_on='user_user_id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19.2 s, sys: 4.76 s, total: 24 s\n",
      "Wall time: 25.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "rbu_drop = ['review_business_id','review_user_id', 'business_hours']\n",
    "rbu_merge = rbu_merge.drop(rbu_drop, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review_date\n",
      "review_review_id\n",
      "review_stars\n",
      "review_text\n",
      "review_useful\n",
      "business_business_id\n",
      "business_is_open\n",
      "business_latitude\n",
      "business_longitude\n",
      "business_name\n",
      "business_review_count\n",
      "business_stars\n",
      "business_state\n",
      "user_name\n",
      "user_review_count\n",
      "user_useful\n",
      "user_user_id\n",
      "user_yelping_since\n"
     ]
    }
   ],
   "source": [
    "for col in rbu_merge: \n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*********\n",
    "### Writing Merged DataFrame Back To Json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def df_to_json(df, path):\n",
    "    new_dict = {}\n",
    "    num_col = len([i for i in df])\n",
    "\n",
    "    with open(r'{}'.format(path), 'w') as writer: \n",
    "\n",
    "        for row in df.itertuples():\n",
    "            \n",
    "            for idx, col in enumerate(df): \n",
    "                if type(row[idx+1]) == int: \n",
    "                    new_dict[col] = int(row[idx+1])\n",
    "                if type(row[idx+1]) == float: \n",
    "                    new_dict[col] = float(row[idx+1])\n",
    "                else: \n",
    "                    new_dict[col] = str(row[idx+1])\n",
    "                \n",
    "            writer.write(json.dumps(new_dict) + '\\n')\n",
    "            new_dict = {}         \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 38s, sys: 24.4 s, total: 6min 2s\n",
      "Wall time: 6min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df_to_json(rbu_merge, rbu_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 54s, sys: 11min 31s, total: 15min 25s\n",
      "Wall time: 31min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "rbu_complete_df = JSON_to_dataFrame(rbu_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "business_business_id     0\n",
       "business_is_open         0\n",
       "business_latitude        0\n",
       "business_longitude       0\n",
       "business_name            0\n",
       "business_review_count    0\n",
       "business_stars           0\n",
       "business_state           0\n",
       "review_date              0\n",
       "review_review_id         0\n",
       "review_stars             0\n",
       "review_text              0\n",
       "review_useful            0\n",
       "user_name                0\n",
       "user_review_count        0\n",
       "user_useful              0\n",
       "user_user_id             0\n",
       "user_yelping_since       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rbu_complete_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unit Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_in_path = root_path + 'test.json' \n",
    "\n",
    "test_df = JSON_to_dataFrame(test_in_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.91 ms, sys: 15.6 ms, total: 19.5 ms\n",
      "Wall time: 34.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "test_out_path = root_path + 'test_out.json' \n",
    "\n",
    "parse_DataFrame1(test_df, test_out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test2_df = JSON_to_dataFrame(test_out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_DataFrame_to_Json(a_df_1, a_df_2): \n",
    "    \n",
    "    df_list_1 = []\n",
    "    df_list_2 = []\n",
    "    catch_list = []\n",
    "    \n",
    "    for row in a_df_1.itertuples():\n",
    "            \n",
    "            for idx, col in enumerate(a_df_1):\n",
    "                df_list_1.append((type(row[idx]), col))\n",
    "                \n",
    "    for row_2 in a_df_2.itertuples():\n",
    "            \n",
    "            for idx_2, col_2 in enumerate(a_df_2):\n",
    "                df_list_2.append((type(row[idx_2]), col_2))\n",
    "                \n",
    "    for x,y in zip(df_list_1, df_list_2): \n",
    "        if x != y: \n",
    "            catch_list.append((x, y))\n",
    "            \n",
    "    for i in catch_list: \n",
    "        print(i)             # dataFrams have different types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# smaller scale test on business set\n",
    "test_DataFrame_to_Json(rbu_merge, rbu_complete_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test on full merged data set\n",
    "test_DataFrame_to_Json(rbu_merge, rbu_complete_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to check file given error message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "check_input_output(test_in_path, test_out_path, log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_input_output(a_test_in_path, a_test_out_path, a_log_path):\n",
    "    line_count = 0\n",
    "    char_count = 0\n",
    "    save1 = \"\"\n",
    "    save2 = []\n",
    "    cnt1 = 0\n",
    "    cnt2 = 0\n",
    "    \n",
    "    with open(a_test_in_path, 'r') as read1, open(a_test_out_path, 'r') as read2: \n",
    "        for x, y in zip(read1, read2):\n",
    "            x = x.strip()\n",
    "            y = y.strip()\n",
    "            \n",
    "            in_line = x.split()\n",
    "            out_line = y.split()\n",
    "            for word1, word2 in zip(in_line, out_line):\n",
    "                \n",
    "                save2.append(word1)\n",
    "                \n",
    "                if cnt1 > 5:\n",
    "                    save2.pop(0)\n",
    "                cnt1 = cnt1 + 1\n",
    "\n",
    "            \n",
    "                word_list = list(word2)\n",
    "                \n",
    "                for char in word_list: \n",
    "                    save1 = save1 + char\n",
    "                    if cnt2 > 70: \n",
    "                        save1 = save1[1:]\n",
    "                    \n",
    "                    if char_count == 139: \n",
    "                        print(save1)\n",
    "                        print(\" \".join(save2))\n",
    "                        \n",
    "                    char_count = char_count + 1\n",
    "                    cnt2 += 1\n",
    "                \n",
    "            line_count = line_count + 1\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(test_out_path, 'r') as reader: \n",
    "    for i in reader: \n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****\n",
    "## GCP Import\n",
    "* login cradentaials provided to work with yelp_db database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install PyMySQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MySql Server is nessisary to connect to GCP\n",
    "* https://dev.mysql.com/downloads/mysql/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pymysql, os, sys, json\n",
    "from IPython.display import clear_output\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read json file\n",
    "file = \"Group1/review.json\"\n",
    "\n",
    "json_data = open(file, encoding=\"utf8\")\n",
    "json_obj = [json.loads(x) for x in json_data.readlines()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num = len(json_obj)\n",
    "print(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# validate data before inserting\n",
    "# TODO: check if string is the right type \n",
    "# TODO: check for duplicate data\n",
    "# TODO: check for bad formats (date)\n",
    "\n",
    "def validate_string(val): \n",
    "    if val != None: \n",
    "            if type(val) is int: \n",
    "                return str(val).encode('utf-8')\n",
    "            else: \n",
    "                return val\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing takes to long so I used multiprocessing to speed up importing to host\n",
    "* executing insert into gcp is atomic\n",
    "\n",
    "this will take over 4 days to import one table!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def insert_review(cursor, item, lock): \n",
    "          \n",
    "    review_id = validate_string(item['review_id'])\n",
    "    business_id = validate_string(item['business_id'])\n",
    "    cool = validate_string(item['cool'])\n",
    "    date = validate_string(item['date'])\n",
    "    funny = validate_string(item['funny'])\n",
    "    stars = validate_string(item['stars'])\n",
    "    text = validate_string(item['text'])\n",
    "    useful = validate_string(item['useful'])\n",
    "    user_id = validate_string(item['user_id'])\n",
    "    \n",
    "    query = \"INSERT INTO review(review_id,business_id,cool,date,funny,stars,text,useful,user_id) VALUES(%s,%s,%s,%s,%s,%s,%s,%s,%s)\"\n",
    "    args = (review_id, business_id, cool, date, funny, stars, text, useful, user_id)\n",
    "    lock.acquire()\n",
    "    cursor.execute(query, args)\n",
    "    lock.release()\n",
    "    \n",
    "    clear_output()\n",
    "    cnt.value = cnt.value + 1\n",
    "    print(cnt.value)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# connect to GCP MySql db insert data into review table\n",
    "con = pymysql.connect(host = '104.198.65.208', user='root', password = 'hackme', db = 'yelp_db')\n",
    "\n",
    "cnt = mp.Value('i', 0)\n",
    "lock = mp.Lock()\n",
    "pool = mp.Pool()\n",
    "\n",
    "\n",
    "try: \n",
    "    with con.cursor() as cursor:\n",
    "        \n",
    "        # parse json data to SQL\n",
    "        # use map to update each row in GCP\n",
    "        result = pool.map([ insert_review(cursor, item, lock) for item in json_obj ])\n",
    "            \n",
    "finally:\n",
    "    con.commit()\n",
    "    con.close()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
