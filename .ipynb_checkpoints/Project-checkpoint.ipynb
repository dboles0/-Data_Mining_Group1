{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Import\n",
    "\n",
    "* working localy trim the data to ensure you can work with it locally <br\\>*(my computer does not have sufficient local memory to join all tables so I use appropriate attributes)*\n",
    "\n",
    "attempted stratagy (merge pd_df until memory bound then print to file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, csv, os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def JSON_to_dataFrame(file): \n",
    "    \n",
    "    fp = open(file, encoding=\"utf8\")\n",
    "    json_obj = [ json.loads(x) for x in fp.readlines() ] \n",
    "    df = pd.DataFrame(json_obj)\n",
    "    fp.close\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def JSON_to_dataFrame_half(file): \n",
    "    \n",
    "    fp = open(file, encoding=\"utf8\")\n",
    "    json_obj = [ json.loads(x) for x in fp.readlines() [-50000:]] \n",
    "    df = pd.DataFrame(json_obj)\n",
    "    fp.close\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = \"/Users/2015mbp16gb256gb/Documents/school/Fall_2019/4502-Data_Mining/Group_Project/Group1/yelp_dataset/\"\n",
    "rbu_path = root_path + \"rbu_merge.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "review_df = JSON_to_dataFrame(root_path + \"review.json\")\n",
    "business_df = JSON_to_dataFrame(root_path + \"business.json\")\n",
    "user_df = JSON_to_dataFrame(root_path + \"user.json\")\n",
    "# tip_df = JSON_to_dataFrame(root_path + \"tip.json\")                     /not used\n",
    "# checkin_df = JSON_to_dataFrame(root_path + \"checkin.json\")             /not used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*********\n",
    "### Merging Files\n",
    "* paired the information down by combining the review business and user json files in to one merged csv file for processing (rbu_merged.csv) (deleted the other files and will be using this one to save space on my computer, if we need additional columns we will have to run this over adding the column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# utility function to make a dict of columns with table name added\n",
    "# used for the renaming of column names\n",
    "def make_columns(lst, base):\n",
    "    ret = {}\n",
    "    for i in lst:\n",
    "        ret[i] = base + '_' + i\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# return dataframe with column names updated to \"table + _column_name\"\n",
    "def process_df(a_df, table): \n",
    "\n",
    "    col_list = [i for i in a_df]\n",
    "    new_dict = make_columns(col_list, table)\n",
    "    return a_df.rename(columns=new_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# review col = ['business_id', 'cool', 'date', 'funny', 'review_id', 'stars', 'text', 'useful', 'user_id']\n",
    "\n",
    "# ***Update review_df*** \n",
    "review_drop = ['cool','funny']\n",
    "review_df = review_df.drop(review_drop, axis=1)\n",
    "\n",
    "review_df = process_df(review_df, 'review')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# business col = \n",
    "# ['address','attributes','business_id','categories','city','hours','is_open',              \n",
    "# 'latitude','longitude','name','postal_code','review_count','stars','state']                              \n",
    "\n",
    "# ***Update business_df***\n",
    "business_drop = ['address', 'attributes', 'categories','city', 'postal_code']\n",
    "business_df = business_df.drop(business_drop, axis=1)\n",
    "\n",
    "business_df = process_df(business_df, 'business')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "rb_merge = pd.merge(review_df, business_df, left_on='review_business_id', right_on='business_business_id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# user col = \n",
    "# ['average_stars','compliment_cool','compliment_cute','compliment_funny','compliment_hot','compliment_list',           \n",
    "# 'compliment_more','compliment_note','compliment_photos','compliment_plain','compliment_profile','compliment_writer', \n",
    "# 'cool','elite','fans','friends','funny','name','review_count','useful','user_id','yelping_since']   \n",
    "\n",
    "# ***Update _user_df***\n",
    "user_drop = ['average_stars','compliment_cool','compliment_cute','compliment_funny','compliment_hot','compliment_list','compliment_more','compliment_note','compliment_photos','compliment_plain','compliment_profile','compliment_writer','cool', 'elite', 'fans','friends','funny']\n",
    "user_df = user_df.drop(user_drop, axis=1)\n",
    "\n",
    "user_df = process_df(user_df, 'user')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "rbu_merge = pd.merge(rb_merge, user_df, left_on='review_user_id', right_on='user_user_id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "rbu_drop = ['review_business_id','review_user_id', 'business_hours']\n",
    "rbu_merge = rbu_merge.drop(rbu_drop, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for col in rbu_merge: \n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*********\n",
    "### Writing Merged DataFrame Back To Json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def df_to_json(df, path):\n",
    "    new_dict = {}\n",
    "    num_col = len([i for i in df])\n",
    "\n",
    "    with open(r'{}'.format(path), 'w') as writer: \n",
    "\n",
    "        for row in df.itertuples():\n",
    "            \n",
    "            for idx, col in enumerate(df): \n",
    "                if type(row[idx+1]) == int: \n",
    "                    new_dict[col] = int(row[idx+1])\n",
    "                if type(row[idx+1]) == float: \n",
    "                    new_dict[col] = float(row[idx+1])\n",
    "                else: \n",
    "                    new_dict[col] = str(row[idx+1])\n",
    "                \n",
    "            writer.write(json.dumps(new_dict) + '\\n')\n",
    "            new_dict = {}         \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df_to_json(rbu_merge, rbu_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create DataFrame from merge file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting plotly\n",
      "  Using cached https://files.pythonhosted.org/packages/f7/05/3c32c6bc85acbd30a18fbc3ba732fed5e48e5f8fd60d2a148877970f4a61/plotly-4.2.1-py2.py3-none-any.whl\n",
      "Collecting retrying>=1.3.3 (from plotly)\n",
      "  Using cached https://files.pythonhosted.org/packages/44/ef/beae4b4ef80902f22e3af073397f079c96969c69b2c7d52a57ea9ae61c9d/retrying-1.3.3.tar.gz\n",
      "Requirement already satisfied, skipping upgrade: six in /opt/anaconda3/lib/python3.7/site-packages (from plotly) (1.12.0)\n",
      "Building wheels for collected packages: retrying\n",
      "  Building wheel for retrying (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for retrying: filename=retrying-1.3.3-cp37-none-any.whl size=11429 sha256=69ab9b5a8b209e818afe5a24979495dd279dc252b033a928d656b05ef29ba562\n",
      "  Stored in directory: /Users/2015mbp16gb256gb/Library/Caches/pip/wheels/d7/a9/33/acc7b709e2a35caa7d4cae442f6fe6fbf2c43f80823d46460c\n",
      "Successfully built retrying\n",
      "Installing collected packages: retrying, plotly\n",
      "Successfully installed plotly-4.2.1 retrying-1.3.3\n",
      "Collecting geopandas==0.3.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0a/0e/8ae74743ed7915ddb7d70cc8dfa8fc0b9b9cc81205c6e288a01915a46192/geopandas-0.3.0-py2.py3-none-any.whl (888kB)\n",
      "\u001b[K     |████████████████████████████████| 890kB 663kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyproj (from geopandas==0.3.0)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/01/56/d93fb4bfa636313955fdbe871e07a1caf1f0957b9fcf4cea7accdbc86e71/pyproj-2.4.0-cp37-cp37m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (13.4MB)\n",
      "\u001b[K     |████████████████████████████████| 13.4MB 2.0MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting fiona (from geopandas==0.3.0)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0d/a1/46bddb57b2161df03614099d5709ee11d6b1d5f08d2a1e34f5974da8a3f9/Fiona-1.8.9.post2-cp37-cp37m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (18.0MB)\n",
      "\u001b[K     |████████████████████████████████| 18.0MB 2.0MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting shapely (from geopandas==0.3.0)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/32/9d1cbfdc94864282be7745a7f7679ca16beb12ec5d71efd12a3f6cc0690a/Shapely-1.6.4.post2-cp37-cp37m-macosx_10_9_x86_64.whl (1.6MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6MB 1.9MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pandas in /opt/anaconda3/lib/python3.7/site-packages (from geopandas==0.3.0) (0.25.1)\n",
      "Collecting descartes (from geopandas==0.3.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/e5/b6/1ed2eb03989ae574584664985367ba70cd9cf8b32ee8cad0e8aaeac819f3/descartes-1.1.0-py3-none-any.whl\n",
      "Collecting munch (from fiona->geopandas==0.3.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/cc/ab/85d8da5c9a45e072301beb37ad7f833cd344e04c817d97e0cc75681d248f/munch-2.5.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: six>=1.7 in /opt/anaconda3/lib/python3.7/site-packages (from fiona->geopandas==0.3.0) (1.12.0)\n",
      "Collecting click-plugins>=1.0 (from fiona->geopandas==0.3.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/e9/da/824b92d9942f4e472702488857914bdd50f73021efea15b4cad9aca8ecef/click_plugins-1.1.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: attrs>=17 in /opt/anaconda3/lib/python3.7/site-packages (from fiona->geopandas==0.3.0) (19.2.0)\n",
      "Collecting cligj>=0.5 (from fiona->geopandas==0.3.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/e4/be/30a58b4b0733850280d01f8bd132591b4668ed5c7046761098d665ac2174/cligj-0.5.0-py3-none-any.whl\n",
      "Requirement already satisfied: click<8,>=4.0 in /opt/anaconda3/lib/python3.7/site-packages (from fiona->geopandas==0.3.0) (7.0)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /opt/anaconda3/lib/python3.7/site-packages (from pandas->geopandas==0.3.0) (2.8.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/anaconda3/lib/python3.7/site-packages (from pandas->geopandas==0.3.0) (2019.3)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /opt/anaconda3/lib/python3.7/site-packages (from pandas->geopandas==0.3.0) (1.17.2)\n",
      "Requirement already satisfied: matplotlib in /opt/anaconda3/lib/python3.7/site-packages (from descartes->geopandas==0.3.0) (3.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/lib/python3.7/site-packages (from matplotlib->descartes->geopandas==0.3.0) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/anaconda3/lib/python3.7/site-packages (from matplotlib->descartes->geopandas==0.3.0) (1.1.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/anaconda3/lib/python3.7/site-packages (from matplotlib->descartes->geopandas==0.3.0) (2.4.2)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib->descartes->geopandas==0.3.0) (41.4.0)\n",
      "Installing collected packages: pyproj, munch, click-plugins, cligj, fiona, shapely, descartes, geopandas\n",
      "Successfully installed click-plugins-1.1.1 cligj-0.5.0 descartes-1.1.0 fiona-1.8.9.post2 geopandas-0.3.0 munch-2.5.0 pyproj-2.4.0 shapely-1.6.4.post2\n",
      "Collecting pyshp==1.2.10\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c2/cc/1851049f2740d09c7bd8e4b464b1b78638723f6173e71d44aea12deca1f8/pyshp-1.2.10.tar.gz (176kB)\n",
      "\u001b[K     |████████████████████████████████| 184kB 1.4MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: pyshp\n",
      "  Building wheel for pyshp (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyshp: filename=pyshp-1.2.10-cp37-none-any.whl size=20468 sha256=1f45513a9dd45796082c387472e2df6785bd4d61f8540e0ca7ebadae61b8c2ca\n",
      "  Stored in directory: /Users/2015mbp16gb256gb/Library/Caches/pip/wheels/ba/1a/67/6a12977f362c33a15edc753daf92c6f01879dbf4db76faf0dd\n",
      "Successfully built pyshp\n",
      "Installing collected packages: pyshp\n",
      "Successfully installed pyshp-1.2.10\n",
      "Collecting shapely==1.6.3\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c0/10/1457c46e20b509108a32a5776141d78d410161dae8ab8da74efe67c530eb/Shapely-1.6.3.tar.gz (223kB)\n",
      "\u001b[K     |████████████████████████████████| 225kB 1.7MB/s eta 0:00:01\n",
      "\u001b[31m    ERROR: Command errored out with exit status 1:\n",
      "     command: /opt/anaconda3/bin/python -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/private/var/folders/dz/jhrl_sb97q3_n3q2900vsl540000gn/T/pip-install-vm1oorx4/shapely/setup.py'\"'\"'; __file__='\"'\"'/private/var/folders/dz/jhrl_sb97q3_n3q2900vsl540000gn/T/pip-install-vm1oorx4/shapely/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base pip-egg-info\n",
      "         cwd: /private/var/folders/dz/jhrl_sb97q3_n3q2900vsl540000gn/T/pip-install-vm1oorx4/shapely/\n",
      "    Complete output (11 lines):\n",
      "    Failed `CDLL(/Library/Frameworks/GEOS.framework/Versions/Current/GEOS)`\n",
      "    Failed `CDLL(/opt/local/lib/libgeos_c.dylib)`\n",
      "    Traceback (most recent call last):\n",
      "      File \"<string>\", line 1, in <module>\n",
      "      File \"/private/var/folders/dz/jhrl_sb97q3_n3q2900vsl540000gn/T/pip-install-vm1oorx4/shapely/setup.py\", line 80, in <module>\n",
      "        from shapely._buildcfg import geos_version_string, geos_version, \\\n",
      "      File \"/private/var/folders/dz/jhrl_sb97q3_n3q2900vsl540000gn/T/pip-install-vm1oorx4/shapely/shapely/_buildcfg.py\", line 185, in <module>\n",
      "        lgeos = load_dll('geos_c', fallbacks=alt_paths)\n",
      "      File \"/private/var/folders/dz/jhrl_sb97q3_n3q2900vsl540000gn/T/pip-install-vm1oorx4/shapely/shapely/_buildcfg.py\", line 161, in load_dll\n",
      "        libname, fallbacks or []))\n",
      "    OSError: Could not find library geos_c or load any of its variants ['/Library/Frameworks/GEOS.framework/Versions/Current/GEOS', '/opt/local/lib/libgeos_c.dylib']\n",
      "    ----------------------------------------\u001b[0m\n",
      "\u001b[31mERROR: Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "try: \n",
    "    !{sys.executable} -m pip install plotly --upgrade\n",
    "    !{sys.executable} -m pip install geopandas==0.3.0\n",
    "    !{sys.executable} -m pip install pyshp==1.2.10\n",
    "    !{sys.executable} -m pip install shapely==1.6.3\n",
    "    !{sys.executable} -m pip install \"notebook>=5.3\" \"ipywidgets>=7.2\"\n",
    "    !{sys.executable} -m pip install chart_studio\n",
    "except: \n",
    "    print(\"already installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running plotly = 4.2.1\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "# clustering imports\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# plotly imports \n",
    "import plotly\n",
    "from plotly import __version__\n",
    "print('running plotly = {}'.format(__version__))\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "\n",
    "import chart_studio.plotly as py \n",
    "import plotly.tools as tls\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.88 s, sys: 410 ms, total: 4.29 s\n",
      "Wall time: 4.31 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "business_df = JSON_to_dataFrame(root_path + \"business.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_oregon = business_df[[]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "x = np.array(business_df['latitude'])\n",
    "y = np.array(business_df['longitude'])\n",
    "\n",
    "iplot([go.Histogram2dContour(x=x, y=y, contours=dict(coloring='haetmap')),\n",
    "       go.Scatter(x=x, y=y, mode='markers', marker=dict(color='white', size=3, opacity=0.3))], show_link=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBSCAN approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reasons for:\n",
    "* Arbitrary\tshape clusters\n",
    "* Handles noise\n",
    "* Single scan\t\n",
    "* Need density parameters for termination condition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coords = business_df.as_matrix(columns=['latitude', 'longitude'])\n",
    "kms_per_radian = 6371.0088\n",
    "e = 1.5 / kms_per_radian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train the dataset\n",
    "dbscan = DBSCAN(eps=e, min_samples = 1, algorithm='ball_tree', metric='haversine')\n",
    "model = dbscan.fit(np.radians(coords))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'DBSCAN' object does not support indexing",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-97b48cc27306>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'DBSCAN' object does not support indexing"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#plot result\n",
    "ptsymb = np.array(['b.','r.','m.','g.','c.','k.','b*','r*','m*','r^'])\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.ylabel('Longitude', fontsize=12)\n",
    "plt.xlabel('Latitude', fontsize=12)\n",
    "\n",
    "for k in unique_labels:\n",
    "    clusters = np.where(labels == k)[0]\n",
    "    \n",
    "    for a_cluster in clusters: \n",
    "        x = array_coordinates[a_cluster]\n",
    "        plt.plot(x[0], x[1], ptsymb[k])\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kmeans Attempt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Kmeans can be sensative to outliers\n",
    "* this can mess up our centroid \n",
    "* we are not to consern since we are only evaluating sections to the us\n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Train model\n",
    "kmeans = KMeans(n_clusters=5).fit(array_coordinates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Plot clusters\n",
    "figure(num=None, figsize=(8, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.scatter(array_coordinates[:, 0], array_coordinates[:, 1], c=kmeans.predict(array_coordinates), s=50, cmap='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "********"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k = 3\n",
    "kmeans = KMeans(n_clusters=k, random_state=0).fit(array_coordinates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "id_label=kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#plot result\n",
    "ptsymb = np.array(['b.','r.','m.','g.','c.','k.','b*','r*','m*','r^'])\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.ylabel('Longitude', fontsize=12)\n",
    "plt.xlabel('Latitude', fontsize=12)\n",
    "\n",
    "for i in range(k):\n",
    "    cluster=np.where(id_label==i)[0]\n",
    "    plt.plot(x.business_latitude[cluster].values, x.business_longitude[cluster].values, ptsymb[i])\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unit Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_in_path = root_path + 'test.json' \n",
    "\n",
    "test_df = JSON_to_dataFrame(test_in_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "test_out_path = root_path + 'test_out.json' \n",
    "\n",
    "parse_DataFrame1(test_df, test_out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test2_df = JSON_to_dataFrame(test_out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_DataFrame_to_Json(a_df_1, a_df_2): \n",
    "    \n",
    "    df_list_1 = []\n",
    "    df_list_2 = []\n",
    "    catch_list = []\n",
    "    \n",
    "    for row in a_df_1.itertuples():\n",
    "            \n",
    "            for idx, col in enumerate(a_df_1):\n",
    "                df_list_1.append((type(row[idx]), col))\n",
    "                \n",
    "    for row_2 in a_df_2.itertuples():\n",
    "            \n",
    "            for idx_2, col_2 in enumerate(a_df_2):\n",
    "                df_list_2.append((type(row[idx_2]), col_2))\n",
    "                \n",
    "    for x,y in zip(df_list_1, df_list_2): \n",
    "        if x != y: \n",
    "            catch_list.append((x, y))\n",
    "            \n",
    "    for i in catch_list: \n",
    "        print(i)             # dataFrams have different types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def JSON_to_dataFrame_2(file): \n",
    "    \n",
    "    fp = open(file, encoding=\"utf8\")\n",
    "    json_obj = [ json.loads(x) for x in fp.readlines() [-500000:]] \n",
    "    df = pd.DataFrame(json_obj)\n",
    "    fp.close\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df1 = JSON_to_dataFrame_2(rbu_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# smaller scale test on business set\n",
    "test_DataFrame_to_Json(rbu_merge, rbu_complete_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test on full merged data set\n",
    "test_DataFrame_to_Json(rbu_merge, rbu_complete_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to check file given error message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "check_input_output(test_in_path, test_out_path, log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_input_output(a_test_in_path, a_test_out_path, a_log_path):\n",
    "    line_count = 0\n",
    "    char_count = 0\n",
    "    save1 = \"\"\n",
    "    save2 = []\n",
    "    cnt1 = 0\n",
    "    cnt2 = 0\n",
    "    \n",
    "    with open(a_test_in_path, 'r') as read1, open(a_test_out_path, 'r') as read2: \n",
    "        for x, y in zip(read1, read2):\n",
    "            x = x.strip()\n",
    "            y = y.strip()\n",
    "            \n",
    "            in_line = x.split()\n",
    "            out_line = y.split()\n",
    "            for word1, word2 in zip(in_line, out_line):\n",
    "                \n",
    "                save2.append(word1)\n",
    "                \n",
    "                if cnt1 > 5:\n",
    "                    save2.pop(0)\n",
    "                cnt1 = cnt1 + 1\n",
    "\n",
    "            \n",
    "                word_list = list(word2)\n",
    "                \n",
    "                for char in word_list: \n",
    "                    save1 = save1 + char\n",
    "                    if cnt2 > 70: \n",
    "                        save1 = save1[1:]\n",
    "                    \n",
    "                    if char_count == 139: \n",
    "                        print(save1)\n",
    "                        print(\" \".join(save2))\n",
    "                        \n",
    "                    char_count = char_count + 1\n",
    "                    cnt2 += 1\n",
    "                \n",
    "            line_count = line_count + 1\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(test_out_path, 'r') as reader: \n",
    "    for i in reader: \n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****\n",
    "## GCP Import\n",
    "* login cradentaials provided to work with yelp_db database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install PyMySQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MySql Server is nessisary to connect to GCP\n",
    "* https://dev.mysql.com/downloads/mysql/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pymysql, os, sys, json\n",
    "from IPython.display import clear_output\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read json file\n",
    "file = \"Group1/review.json\"\n",
    "\n",
    "json_data = open(file, encoding=\"utf8\")\n",
    "json_obj = [json.loads(x) for x in json_data.readlines()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num = len(json_obj)\n",
    "print(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# validate data before inserting\n",
    "# TODO: check if string is the right type \n",
    "# TODO: check for duplicate data\n",
    "# TODO: check for bad formats (date)\n",
    "\n",
    "def validate_string(val): \n",
    "    if val != None: \n",
    "            if type(val) is int: \n",
    "                return str(val).encode('utf-8')\n",
    "            else: \n",
    "                return val\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing takes to long so I used multiprocessing to speed up importing to host\n",
    "* executing insert into gcp is atomic\n",
    "\n",
    "this will take over 4 days to import one table!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def insert_review(cursor, item, lock): \n",
    "          \n",
    "    review_id = validate_string(item['review_id'])\n",
    "    business_id = validate_string(item['business_id'])\n",
    "    cool = validate_string(item['cool'])\n",
    "    date = validate_string(item['date'])\n",
    "    funny = validate_string(item['funny'])\n",
    "    stars = validate_string(item['stars'])\n",
    "    text = validate_string(item['text'])\n",
    "    useful = validate_string(item['useful'])\n",
    "    user_id = validate_string(item['user_id'])\n",
    "    \n",
    "    query = \"INSERT INTO review(review_id,business_id,cool,date,funny,stars,text,useful,user_id) VALUES(%s,%s,%s,%s,%s,%s,%s,%s,%s)\"\n",
    "    args = (review_id, business_id, cool, date, funny, stars, text, useful, user_id)\n",
    "    lock.acquire()\n",
    "    cursor.execute(query, args)\n",
    "    lock.release()\n",
    "    \n",
    "    clear_output()\n",
    "    cnt.value = cnt.value + 1\n",
    "    print(cnt.value)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# connect to GCP MySql db insert data into review table\n",
    "con = pymysql.connect(host = '104.198.65.208', user='root', password = 'hackme', db = 'yelp_db')\n",
    "\n",
    "cnt = mp.Value('i', 0)\n",
    "lock = mp.Lock()\n",
    "pool = mp.Pool()\n",
    "\n",
    "\n",
    "try: \n",
    "    with con.cursor() as cursor:\n",
    "        \n",
    "        # parse json data to SQL\n",
    "        # use map to update each row in GCP\n",
    "        result = pool.map([ insert_review(cursor, item, lock) for item in json_obj ])\n",
    "            \n",
    "finally:\n",
    "    con.commit()\n",
    "    con.close()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
