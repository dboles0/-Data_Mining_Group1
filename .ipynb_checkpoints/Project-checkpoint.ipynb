{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Import\n",
    "\n",
    "* working localy trim the data to ensure you can work with it locally <br\\>*(my computer does not have sufficient local memory to join all tables so I use appropriate attributes)*\n",
    "\n",
    "attempted stratagy (merge pd_df until memory bound then print to file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json, csv, os\n",
    "import pandas as pd\n",
    "\n",
    "def JSON_to_dataFrame(file): \n",
    "    fp = open(file, encoding=\"utf8\")\n",
    "    df = pd.DataFrame([json.loads(x) for x in fp.readlines()])\n",
    "    fp.close\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_path = \"/Users/2015mbp16gb256gb/Documents/school/Fall_2019/4502-Data_Mining/Group_Project/Group1/yelp_dataset/\"\n",
    "os.path.exists(root_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "review_df = JSON_to_dataFrame(root_path + \"review.json\")\n",
    "business_df = JSON_to_dataFrame(root_path + \"business.json\")\n",
    "user_df = JSON_to_dataFrame(root_path + \"user.json\")\n",
    "# tip_df = JSON_to_dataFrame(root_path + \"tip.json\")                     /not used\n",
    "# checkin_df = JSON_to_dataFrame(root_path + \"checkin.json\")             /not used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*********\n",
    "## Merging Files\n",
    "* paired the information down by combining the review business and user json files in to one merged csv file for processing (rbu_merged.csv) (deleted the other files and will be using this one to save space on my computer, if we need additional columns we will have to run this over adding the column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# utility function to make a dict of columns with table name added\n",
    "# used for the renaming of column names\n",
    "def make_columns(lst, base):\n",
    "    ret = {}\n",
    "    for i in lst:\n",
    "        ret[i] = base + '_' + i\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# return dataframe with column names updated to \"table + _column_name\"\n",
    "def process_df(a_df, table): \n",
    "\n",
    "    col_list = [i for i in a_df]\n",
    "    new_dict = make_columns(col_list, table)\n",
    "    return a_df.rename(columns=new_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# review col = ['business_id', 'cool', 'date', 'funny', 'review_id', 'stars', 'text', 'useful', 'user_id']\n",
    "\n",
    "# ***Update review_df*** \n",
    "review_drop = ['cool','funny']\n",
    "review_df = review_df.drop(review_drop, axis=1)\n",
    "\n",
    "review_df = process_df(review_df, 'review')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# business col = \n",
    "# ['address','attributes','business_id','categories','city','hours','is_open',              \n",
    "# 'latitude','longitude','name','postal_code','review_count','stars','state']                              \n",
    "\n",
    "# ***Update business_df***\n",
    "business_drop = ['address', 'attributes', 'categories','city', 'postal_code']\n",
    "business_df = business_df.drop(business_drop, axis=1)\n",
    "\n",
    "business_df = process_df(business_df, 'business')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rb_merge = pd.merge(review_df, business_df, left_on='review_business_id', right_on='business_business_id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# user col = \n",
    "# ['average_stars','compliment_cool','compliment_cute','compliment_funny','compliment_hot','compliment_list',           \n",
    "# 'compliment_more','compliment_note','compliment_photos','compliment_plain','compliment_profile','compliment_writer', \n",
    "# 'cool','elite','fans','friends','funny','name','review_count','useful','user_id','yelping_since']   \n",
    "\n",
    "# ***Update _user_df***\n",
    "user_drop = ['average_stars','compliment_cool','compliment_cute','compliment_funny','compliment_hot','compliment_list','compliment_more','compliment_note','compliment_photos','compliment_plain','compliment_profile','compliment_writer','cool', 'elite', 'fans','friends','funny']\n",
    "user_df = user_df.drop(user_drop, axis=1)\n",
    "\n",
    "user_df = process_df(user_df, 'user')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rbu_merge = pd.merge(rb_merge, user_df, left_on='review_user_id', right_on='user_user_id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbu_drop = ['review_business_id','review_user_id']\n",
    "rbu_merge = rbu_merge.drop(rbu_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "try: \n",
    "    out = rbu_merge.to_json()\n",
    "except OSError as e:\n",
    "    print(\"OS error: {0}\".format(e))\n",
    "except:\n",
    "    print(\"Unexpected error:\", sys.exc_info()[0])\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbu_path = root_path + \"rbu_merge.json\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26min 54s, sys: 42 s, total: 27min 36s\n",
      "Wall time: 28min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "with open('rbu_merge.json', 'w') as fp:\n",
    "    for i in out: \n",
    "        fp.write(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "******\n",
    "## Convert Merged File into DataFrame (using rbu_merge.json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df = JSON_to_dataFrame(root_path + \"rbu_merge.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "******\n",
    "## Convert Merged File into DataFrame (using rbu_merge.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try: \n",
    "    df = pd.read_csv(\"../Group_Project/Group1/rbu_merge.csv\", dtype={'review_business_id':str, 'review_date':str, 'review_review_id':str, 'review_stars':float, 'review_text':str, 'review_useful':int, 'review_user_id':str, 'business_business_id':str, 'business_city':str, 'business_is_open':int, 'business_latitude':float, 'business_longitude':float, 'business_name':str, 'business_review_count':int, 'business_stars':float, 'business_state':str, 'user_average_stars':float, 'user_fans':int, 'user_friends':str, 'user_name':str, 'user_review_count':int, 'user_useful':int, 'user_user_id':str,'user_yelping_since':str})\n",
    "except: \n",
    "    del()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****\n",
    "## GCP Import\n",
    "* login cradentaials provided to work with yelp_db database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install PyMySQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MySql Server is nessisary to connect to GCP\n",
    "* https://dev.mysql.com/downloads/mysql/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pymysql, os, sys, json\n",
    "from IPython.display import clear_output\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read json file\n",
    "file = \"Group1/review.json\"\n",
    "\n",
    "json_data = open(file, encoding=\"utf8\")\n",
    "json_obj = [json.loads(x) for x in json_data.readlines()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num = len(json_obj)\n",
    "print(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# validate data before inserting\n",
    "# TODO: check if string is the right type \n",
    "# TODO: check for duplicate data\n",
    "# TODO: check for bad formats (date)\n",
    "\n",
    "def validate_string(val): \n",
    "    if val != None: \n",
    "            if type(val) is int: \n",
    "                return str(val).encode('utf-8')\n",
    "            else: \n",
    "                return val\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing takes to long so I used multiprocessing to speed up importing to host\n",
    "* executing insert into gcp is atomic\n",
    "\n",
    "this will take over 4 days to import one table!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def insert_review(cursor, item, lock): \n",
    "          \n",
    "    review_id = validate_string(item['review_id'])\n",
    "    business_id = validate_string(item['business_id'])\n",
    "    cool = validate_string(item['cool'])\n",
    "    date = validate_string(item['date'])\n",
    "    funny = validate_string(item['funny'])\n",
    "    stars = validate_string(item['stars'])\n",
    "    text = validate_string(item['text'])\n",
    "    useful = validate_string(item['useful'])\n",
    "    user_id = validate_string(item['user_id'])\n",
    "    \n",
    "    query = \"INSERT INTO review(review_id,business_id,cool,date,funny,stars,text,useful,user_id) VALUES(%s,%s,%s,%s,%s,%s,%s,%s,%s)\"\n",
    "    args = (review_id, business_id, cool, date, funny, stars, text, useful, user_id)\n",
    "    lock.acquire()\n",
    "    cursor.execute(query, args)\n",
    "    lock.release()\n",
    "    \n",
    "    clear_output()\n",
    "    cnt.value = cnt.value + 1\n",
    "    print(cnt.value)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# connect to GCP MySql db insert data into review table\n",
    "con = pymysql.connect(host = '104.198.65.208', user='root', password = 'hackme', db = 'yelp_db')\n",
    "\n",
    "cnt = mp.Value('i', 0)\n",
    "lock = mp.Lock()\n",
    "pool = mp.Pool()\n",
    "\n",
    "\n",
    "try: \n",
    "    with con.cursor() as cursor:\n",
    "        \n",
    "        # parse json data to SQL\n",
    "        # use map to update each row in GCP\n",
    "        result = pool.map([ insert_review(cursor, item, lock) for item in json_obj ])\n",
    "            \n",
    "finally:\n",
    "    con.commit()\n",
    "    con.close()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
